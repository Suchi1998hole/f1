# -*- coding: utf-8 -*-
"""f1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1he08xQnRcafYAEmYsMQET4AmnpdVy7MA
"""



import pandas as pd
import numpy as np
import seaborn as sns
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import datetime as dt
import pandas as pd
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc
from sqlalchemy import create_engine
from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)
simplefilter("ignore", category=FutureWarning)

# Connection to SQLite
engine = create_engine(f"sqlite:///D:/f1/f1.db")

# Querying the database

races_query = "SELECT * FROM races"
circuits_query = "SELECT * FROM circuits"
constructor_results_query = "SELECT * FROM constructor_results"
constructor_standings_query = "SELECT * FROM constructor_standings"
constructors_query = "SELECT * FROM constructors"
driver_standings_query = "SELECT * FROM driver_standings"
drivers_query = "SELECT * FROM drivers"
lap_times_query = "SELECT * FROM lap_times"
pit_stops_query = "SELECT * FROM pit_stops"
qualifying_query = "SELECT * FROM qualifying"
results_query = "SELECT * FROM results"
seasons_query = "SELECT * FROM seasons"
status_query = "SELECT * FROM status"

#Data import Method 1:
#converting to pandas dataframes
circuits = pd.read_sql_query(circuits_query, engine)
constructor_results = pd.read_sql_query(constructor_results_query, engine)
constructor_standings = pd.read_sql_query(constructor_standings_query, engine)
constructors_df = pd.read_sql_query(constructors_query, engine)
drivers_df = pd.read_sql_query(drivers_query, engine)
race_df = pd.read_sql_query(races_query, engine)
pit_stops = pd.read_sql_query(pit_stops_query, engine)
qualifying = pd.read_sql_query(qualifying_query, engine)
driver_standings = pd.read_sql_query(driver_standings_query, engine)
status = pd.read_sql_query(status_query, engine)
seasons = pd.read_sql_query(seasons_query, engine)
results_df = pd.read_sql_query(results_query, engine)
lap_times = pd.read_sql_query(lap_times_query, engine)

#Data import Method 2:
#csv to pandas dataframe
circuits_df = pd.read_csv('circuits.csv')
constructor_results_df = pd.read_csv('constructor_results.csv')
constructor_standings_df = pd.read_csv('constructor_standings.csv')
constructors_df = pd.read_csv('constructors.csv')
drivers_df = pd.read_csv('drivers.csv')
race_df = pd.read_csv('races.csv')
pit_stops_df = pd.read_csv('pit_stops.csv')
qualifying_df = pd.read_csv('qualifying.csv')
driver_standings_df = pd.read_csv('driver_standings.csv')
status_df = pd.read_csv('status.csv')
seasons_df = pd.read_csv('seasons.csv')
results_df = pd.read_csv('results.csv')
lap_times_df = pd.read_csv('lap_times.csv')
sprint_results_df = pd.read_csv('sprint_results.csv')

"""DATA ANALYSIS"""

#Example: display all columns results from results.csv
pd.get_option("display.max_columns",None)
#print(results_df)

# Starting grid, final position of every driver, constructor and status of each race
#print("Driver result of a race")
#print(results_df.head())

#Example: perform left join on constructors and results
team = constructors_df.merge(results_df,on='constructorId',how = 'left')
#print(team)

#column extraction from above df teams
best_teams = team[['name','points','raceId']]
#grouping based on team names and counting unique races
best_teams = best_teams.groupby('name')['raceId'].nunique().sort_values(ascending=False).reset_index(name = 'races')
best_teams = best_teams[best_teams['races'] >= 100]
best_teams.head(5)

"""VISUALIZATION"""

#Visualization 1: AVERAGE CONSTRUCTORS POINTS PER RACE

#calculations
func = lambda x: x.points.sum()/x.raceId.nunique()
data = team[team['name'].isin(best_teams.name)].groupby('name').apply(func).sort_values(ascending=False).reset_index(name = 'points_per_race')
data.head(10)

#plotting
fig = go.Figure(
    data=[go.Bar(x = data.name, y=data['points_per_race'])],
    layout_title_text="Constructor's Points per Race"

)
fig.update_xaxes(showgrid=False)
fig.update_yaxes(showgrid=False)
fig.update_traces(textfont_size=20,
                  marker=dict(line=dict(color='blue', width=2)))
fig.show()

#Visualization 2: DRIVER NATIONALITIES
driver_nationality = drivers_df.groupby('nationality')['nationality'].count().sort_values(ascending = False).reset_index(name = 'number of drivers')
fig = go.Figure(data=[go.Pie(labels=driver_nationality.nationality.head(10), values=driver_nationality['number of drivers'])])
fig.update_traces(textfont_size=20,
                  marker=dict(line=dict(color='#000000', width=2)))
fig.update_layout(
    title="Historical Driver Nationality Distribution")
fig.show()

#Visualization 3: MOST WINS BY DRIVER IN A SEASON

#calculation
driver_position = drivers_df.merge(driver_standings_df,left_on='driverId',right_on='driverId',how = 'left')
driver_position = driver_position.merge(race_df,on = 'raceId',how = 'left')
positions = driver_position[driver_position['position'] == 1].groupby(
    ['surname','year'])['wins'].max().sort_values(ascending=False).reset_index(name = 'Wins')
positions.head(20)
positions.year=pd.to_datetime(positions['year'].astype(int), format='%Y')
positions.year = positions.year.dt.year
positions.rename(columns={'surname':'name'},inplace=True)
positions.Wins = positions.Wins.astype('int64')
positions.head(11)

#plotting
fig = px.scatter(positions.head(30), x="year", y="Wins", color="name",
                 title="Most wins by a driver in a single season",size = 'Wins')
fig.update_traces(textfont_size=20,
                  marker=dict(line=dict(color='#000000', width=2)))
fig.update_xaxes(showgrid=False)
fig.show()

"""In the above visualization,Max Verstappen has had the highest wins(19) in year 2023.(HOVER OVER THE GRAPH)"""

#Visualization 4: NUMBER OF FORMULA 1 RACES PER YEAR
# Group data by 'year' and count the number of races
res_df = results_df[['raceId', 'driverId', 'constructorId', 'grid', 'positionOrder']].copy()
race_df = race_df[race_df["year"] >= 1982]
df = pd.merge(race_df, res_df, on='raceId')
races_per_year = df.groupby('year')['raceId'].nunique()

# Create a bar plot
plt.figure(figsize=(12, 4))  # You can adjust the size as per your preference
races_per_year.plot(kind='bar', color='skyblue')

plt.title('Number of Formula 1 Races per Year')
plt.xlabel('Year')
plt.xticks(fontsize = 8)
plt.ylabel('Number of Races')
plt.xticks(rotation=45)  # Rotates the x-axis labels for better readability
plt.grid(axis='y')

# Display the plot
#plt.show()

#Visualization 5: TOP 15 F1 TEAMS(CONSTRUCTORS)BY THE NUMBER OF RACES THEY HAVE PARTICIPATED SINCE 1982
# Group data by 'constructorId' and count unique races
races_per_team = df.groupby('constructorId')['raceId'].nunique()

# Create a dictionary to map 'constructorId' to 'name'
constructor_name_dict = pd.Series(constructors_df.name.values, index=constructors_df.constructorId).to_dict()

# Replace 'constructorId' with corresponding 'name' in races_per_team
races_per_team.index = races_per_team.index.map(constructor_name_dict)

# Sort the data from most races to least races and select top 15
races_per_team_sorted = races_per_team.sort_values(ascending=False).head(15)

# Create a bar plot
plt.figure(figsize=(5,5))  # Adjust the figure size to your preference
colors = plt.cm.Spectral(np.linspace(0, 1, len(races_per_team_sorted))) # Using Spectral colormap
races_per_team_sorted.plot(kind='barh', color=colors)

plt.title('Top 15 Formula 1 Teams by Number of Races since 1982')
plt.ylabel('Team Name')
plt.xlabel('Number of Races')
plt.grid(axis='x')

# Reverse the order of the y-axis so the team with most races is on top
plt.gca().invert_yaxis()

# Display the plot
#plt.show()

#Visualization 6: TOP 15 F1 TEAMS(DRIVERS)BY THE NUMBER OF RACES THEY HAVE PARTICIPATED SINCE 1982
# Group data by 'driverId' and count unique races
races_per_team = df.groupby('driverId')['raceId'].nunique()

# Create a dictionary to map 'driverId' to 'name'
driver_name_dict = pd.Series(drivers_df.surname.values, index=drivers_df.driverId).to_dict()

# Replace 'driverId' with corresponding 'name' in races_per_team
races_per_team.index = races_per_team.index.map(driver_name_dict)

# Sort the data from most races to least races and select top 15
races_per_team_sorted = races_per_team.sort_values(ascending=False).head(15)

# Create a bar plot
plt.figure(figsize=(5, 5))  # Adjust the figure size to your preference
colors = plt.cm.Spectral(np.linspace(0, 1, len(races_per_team_sorted))) # Using Spectral colormap
races_per_team_sorted.plot(kind='barh', color=colors)

plt.title('Top 15 Formula 1 Drivers by Number of Races since 1982')
plt.ylabel('Team Name')
plt.xlabel('Number of Races')
plt.grid(axis='x')

# Reverse the order of the y-axis so the team with most races is on top
plt.gca().invert_yaxis()

# Display the plot
#plt.show()

"""Data collection for training model
Extracting raceId, year, round, circuitId from races dataframe to train model
"""

race_df = race_df[["raceId", "year", "round", "circuitId"]].copy()
#print(race_df)

"""Data Preprocessing"""

#sorting data by year and using data after year 1982 to have relevant data to newer f1 period
race_df = race_df.sort_values(by=['year', 'round'])
race_df = race_df[race_df["year"] >= 1982]
#print(race_df)

"""Extracting raceId, driverId, constructorId, grid (starting postion), positionOrder (finishing position) from results dataframe to train model"""

res_df = results_df[['raceId', 'driverId', 'constructorId', 'grid', 'positionOrder']].copy()
#print(res_df)

#check duplicates in race data
duplicates = race_df.duplicated()
num_duplicates = duplicates.sum()
#print(f"Number of duplicate rows: {num_duplicates}")
#print(race_df)

"""Merge race and results data: to have information about the year, the round, and the circuit for a specific race. We checked in above cell that we have no duplicate raceId, so we can proceed with merging the two datasets."""

#merged columns: raceId, driverId, constructorId, grid, positionOrder, round, year
df = pd.merge(race_df, res_df, on='raceId')
#print(df)

#check if missing values present in any columns
df.dtypes
df.isna().sum()



"""Feature Engineering: creating and adding a new feature "Top 3 Finish" to specify whether a driver has finished in the top 3 for every race."""

# Creating a column for Top 3 Finish
df['Top 3 Finish'] = df['positionOrder'].le(3).astype(int)

# Display the first few rows to confirm the new column
#print(df)

# To confirm we have the correct data
numberRace = df["raceId"].nunique()
#print(f"Number of unique drivers: {numberRace}")

"""IMPORTANT: Following are past performance calculations to predict future race results

Calculating: 1. Top 3 Driver finishes % last year
2. Top 3 Driver finishes % current year till previous race
3. Top 3 Constructors finishes % last year
4. Top 3 Constructors finishes % current year till previous race

Merging: Following 2 columns will be added to our data frame:
1.Driver Top 3 Finish Percentage (Last Year)
2.Constructor Top 3 Finish Percentage (Last Year)
"""

# Calculating the total number of races and top 3 finishes for each driver in each year
driver_yearly_stats = df.groupby(['year', 'driverId']).agg(
    Total_Races=('raceId', 'nunique'),
    Top_3_Finishes=('Top 3 Finish', 'sum')
).reset_index()

#print("Driver annual stats")
#print(driver_yearly_stats)
#example reading: row 4:driver with driverId 117 had been in top 3 postitions 4 times over the year 1982 and took part in 16 races.

#percentage calculation for top 3 finishes for each driver in each year
#formula: (top 3 finishes / total races)*100
driver_yearly_stats['Driver Top 3 Finish Percentage (This Year)'] = (driver_yearly_stats['Top_3_Finishes'] / driver_yearly_stats['Total_Races']) * 100

# While calculating percentage for last year, we need to shift the 'year' feature by 1.
# Shifting the driver percentages to the next year for last year's data
driver_last_year_stats = driver_yearly_stats.copy()
driver_last_year_stats['year'] += 1
driver_last_year_stats = driver_last_year_stats.rename(columns={'Driver Top 3 Finish Percentage (This Year)': 'Driver Top 3 Finish Percentage (Last Year)'})

#appending the new column to data frame
df = pd.merge(df, driver_last_year_stats[['year', 'driverId', 'Driver Top 3 Finish Percentage (Last Year)']], on=['year', 'driverId'], how='left')

# Checking the merged data
#print("New dataframe")
#print(df[df["year"]>=1983])
#setting filter to 1983 as top 3 finishes column

#print(df[df["year"]>=1982])
#Drivers Top 3 Finish Percentage (Last Year) column has NaNs since we are taking values from 1983

# We need to calculate mean of constructors stats as 1 team has 2 drivers. Calculating for last year
constructor_last_year_stats = df.groupby(['year', 'constructorId', 'round']).agg(
    Sum_Top_3_Finishes_Last_Year=('Driver Top 3 Finish Percentage (Last Year)', 'sum')
).reset_index()

#print("Constructor annual stats")
#print(constructor_last_year_stats)

# Calculating the percentage of top 3 finishes for each constructor last year
constructor_last_year_stats['Constructor Top 3 Finish Percentage (Last Year)'] = constructor_last_year_stats["Sum_Top_3_Finishes_Last_Year"]/2

df = pd.merge(df, constructor_last_year_stats[['year', 'constructorId', 'round', 'Constructor Top 3 Finish Percentage (Last Year)']], on=['year', 'constructorId', 'round'], how='left')

# Checking the merged data
#print("New dataframe")
#print(df[df["year"]>=1983])

"""Driver: Percentage of finishing in top 3 for current year till last race"""

# Function to calculate the top 3 finish percentage before the current round in current year for drivers
def calculate_driver_top_3_percentage_before_round(row, df):
    # Filter for races in the same year, for the same driver, but in earlier rounds
    previous_races = df[(df['year'] == row['year']) & (df['driverId'] == row['driverId']) & (df['round'] < row['round'])]
    if len(previous_races) == 0:
      return pd.NA

    total_races = previous_races['raceId'].nunique()
    top_3_finishes = previous_races['Top 3 Finish'].sum()

    # Calculate the percentage
    return (top_3_finishes / total_races) * 100 if total_races > 0 else pd.NA

# Apply the function to each row in the DataFrame
df['Driver Top 3 Finish Percentage (This Year till last race)'] = df.apply(lambda row: calculate_driver_top_3_percentage_before_round(row, df), axis=1)

"""Constructor(Team): Percentage of finishing in top 3 for current year till last race"""

# Calculating mean of top 3 finishes percentages for the two drivers in each constructor this year
constructor_this_year_stats = df.groupby(['year', 'constructorId', 'round']).agg(
    Sum_Top_3_Finishes_This_Year=('Driver Top 3 Finish Percentage (This Year till last race)', 'sum')
).reset_index()

#print("Constructor annual stats")
#print(constructor_this_year_stats)

# Calculating the percentage of top 3 finishes for each constructor this year
constructor_this_year_stats['Constructor Top 3 Finish Percentage (This Year till last race)'] = constructor_this_year_stats["Sum_Top_3_Finishes_This_Year"]/2

df = pd.merge(df, constructor_this_year_stats[['year', 'constructorId', 'round', 'Constructor Top 3 Finish Percentage (This Year till last race)']], on=['year', 'constructorId', 'round'], how='left')

# Checking the merged data
#print("New dataframe")
#print(df[df["year"]>=1983])

"""Past Performance Features: Average Finishing position

Driver: Average finishing position for past year
"""

# Calculating the total number of races and top 3 finishing position for each driver in each year
driver_yearly_stats = df.groupby(['year', 'driverId']).agg(
    Total_Races=('raceId', 'nunique'),
    Avg_position=('positionOrder', 'mean')
).reset_index()

#print("Driver annual stats")
#print(driver_yearly_stats)

# Calculating the percentage of top 3 finishes for each driver in each year
driver_yearly_stats['Driver Avg position (This Year)'] = driver_yearly_stats['Avg_position']

# Shifting the driver percentages to the next year for last year's data
driver_last_year_stats = driver_yearly_stats.copy()
driver_last_year_stats['year'] += 1
driver_last_year_stats = driver_last_year_stats.rename(columns={'Driver Avg position (This Year)': 'Driver Avg position (Last Year)'})

df = pd.merge(df, driver_last_year_stats[['year', 'driverId', 'Driver Avg position (Last Year)']], on=['year', 'driverId'], how='left')

# Checking the merged data
#print("New dataframe")
#print(df[df["year"]>=1983])

"""Constructor(Team): Average finishing position for past year
Here by constructor average finishing position we mean the average finishing position of both drivers in the team.


"""

# Calculating mean of top 3 finishes percentages for the two drivers in each constructor last year
constructor_last_year_stats = df.groupby(['year', 'constructorId', 'round']).agg(
    sum_position_last_year=('Driver Avg position (Last Year)', 'sum')
).reset_index()

#print("Constructor annual stats")
#print(constructor_last_year_stats)

# Calculating the percentage of top 3 finishes for each constructor last year
constructor_last_year_stats['Constructor Avg position (Last Year)'] = constructor_last_year_stats["sum_position_last_year"]/2

df = pd.merge(df, constructor_last_year_stats[['year', 'constructorId', 'round', 'Constructor Avg position (Last Year)']], on=['year', 'constructorId', 'round'], how='left')

# Checking the merged data
#print("New dataframe")
#print(df[df["year"]>=1983])

"""Average finishing position of driver for current year till last race"""

def calculate_driver_avg_position_before_round(row, df):
    # Filter for races in the same year, for the same driver, but in earlier rounds
    previous_races = df[(df['year'] == row['year']) & (df['driverId'] == row['driverId']) & (df['round'] < row['round'])]
    if len(previous_races) == 0:
      return pd.NA
    # Calculate the total races and sum of positions
    total_races = previous_races['raceId'].nunique()
    positionSum = previous_races['positionOrder'].sum()

    # Calculate average position
    return (positionSum / total_races) if total_races > 0 else pd.NA

# Apply the function to each row in the DataFrame
df['Driver Average Position (This Year till last race)'] = df.apply(lambda row: calculate_driver_avg_position_before_round(row, df), axis=1)

"""Average finishing position of constructor for current year till last race"""

# Calculating mean of top 3 finishes percentages for the two drivers in each constructor this year
constructor_this_year_stats = df.groupby(['year', 'constructorId', 'round']).agg(
    sum_Position_Constructor = ('Driver Average Position (This Year till last race)', 'sum')
).reset_index()

#print("Constructor annual stats")
#print(constructor_this_year_stats)

# Calculating the percentage of top 3 finishes for each constructor this year
constructor_this_year_stats['Constructor Average Position (This Year till last race)'] = constructor_this_year_stats["sum_Position_Constructor"]/2

df = pd.merge(df, constructor_this_year_stats[['year', 'constructorId', 'round', 'Constructor Average Position (This Year till last race)']], on=['year', 'constructorId', 'round'], how='left')

# Checking the merged data
#print("New dataframe")
#print(df[df["year"]>=1983])

#print(df[(df["year"] == 2024)& (df["round"] > 3) ].tail(30))

nan_counts = df.isna().sum()
#print(nan_counts)

"""1.The NA in top 3 Finish Percentage Last Year represents that the driver did not participate in F1 last year.
2.The NA in Driver Top 3 Finish Percentage (This Year till last race) represents that it is the first round of the year (so no previous data) or this is the first round that this driver is participating in that year.
"""

# # Creating a column called Top 3 Finish, where position order is less than or equal to 3
# df['Top 3 Finish'] = df['positionOrder'].le(3).astype(int)
# #print(df)

# Creating a column for Top 3 Finish
df['Top 3 Finish'] = df['positionOrder'].le(3).astype(int)

# Display the first few rows to confirm the new column
#print(df)

# To confirm we have the correct data
numberRace = df["raceId"].nunique()
#print(f"Number of unique drivers: {numberRace}")

#drop raceId to prevent data leakage
df_final = df.drop(labels=["raceId"], axis=1)

#final shape
#print("Number of rows in total:", df_final.shape[0])

# Total rows where 'year' is not 1982 before dropping NaN values
initial_count = len(df_final[df_final['year'] != 1982])
#print(initial_count)

# Drop rows with NaN values
df_final = df_final.dropna()
# Total rows where 'year' is not 1982 after dropping NaN values
final_count = len(df_final[df_final['year'] != 1982])
#print(final_count)

#total number of rows dropped
rows_dropped = initial_count - final_count
#print(rows_dropped)

#storing data frame with positionOrder feature as a back up before dropping it
df_final_keepPositionOrder = df_final.copy()
#drop Position Order from data frame
df_final = df_final.drop(["positionOrder"], axis = 1)
#print(df_final)

"""Heatmap"""

# heatmap
plt.figure(figsize=(10,7))
sns.heatmap(df_final.corr(), annot=True, mask = False, annot_kws={"size": 7})
#plt.show()

"""in the above heatmap, we have strong correlation between 'Driver Top 3 finish percentages' of drivers and 'Constructor Top 3 finish percentages' of constructors with our target variable: 'Top 3 finish', whereas 'grid', 'Driver Average Position', 'Constructor Average Position' have negative correlations with our target variable.

Analyzing the heatmap to identify which features contribute the most to the "Top 3 Finish" target variable

Observations from the Heatmap:
"Top 3 Finish" (target variable) :
Positive values (closer to 1) indicate a strong positive correlation.
Negative values (closer to -1) indicate a strong negative correlation.
Values near 0 indicate little to no correlation.

*positive correlations*
1.Driver Top 3 Finish Percentage (Last Year):
Correlation ~ 0.46 (moderately strong positive).
This suggests that past top 3 finishes by the driver have a significant impact on predicting future top 3 finishes.

2.Constructor Top 3 Finish Percentage (Last Year):
Correlation ~ 0.47 (moderately strong positive).
Performance of the constructor last year is also an important predictor.

3.Driver Top 3 Finish Percentage (This Year till last race):
Correlation ~ 0.53 (strong positive).
Recent driver performance this year is even more influential.

4.Constructor Top 3 Finish Percentage (This Year till last race):
Correlation ~ 0.53 (strong positive).
Similar to driver performance, recent constructor success this year is important.

*Negative correlations*
1.Driver Average Position (Last Year):
Correlation ~ -0.36.

2.Constructor Average Position (Last Year):
Correlation ~ -0.32 (negative correlation).

3.Grid
Correlation ~ -0.42
"""

#tabular format
correlations = df_final.corr()['Top 3 Finish'].sort_values(ascending=False)
#print(correlations)

"""One Hot Encoding:for columns: circuitId, driverId, constructorId using pandas get_dummies method"""

df_final_encoded = pd.get_dummies(df_final, columns=['circuitId', 'driverId', 'constructorId'])

# Create a list of columns excluding the one to append it at end as it is our target variable
cols = [col for col in df_final_encoded.columns if col != 'Top 3 Finish']
#print(cols)

# Append here to the end of the DataFrame
df_final_encoded = df_final_encoded[cols + ['Top 3 Finish']]

cols = [col for col in df_final_encoded.columns]
#print(cols)

#print(df_final_encoded)
#print(df_final_encoded.shape)

#display any row to see structure of data frame
#print(df_final_encoded.iloc[58])

df_final_encoded.head(25)

"""Data Modelling

Model 1: Predicting F1 podiums without qualfying data:
Trianing data: year 1983 - 2008

Validation data: year 2009 - 2016

Test data: year 2017 - 2023
"""

#features taken to train model: year, round, circuit, constructor, race,  percentages of finishing in top 3 for the past year for drivers,  percentages of finishing in top 3 for the past year for constructors
# drop features detected in heatmap that dont correlate to top 3 finishes target variable
df_new = df_final_encoded.drop(["grid", "Driver Top 3 Finish Percentage (This Year till last race)", "Constructor Top 3 Finish Percentage (This Year till last race)", "Driver Average Position (This Year till last race)", "Constructor Average Position (This Year till last race)", "Driver Average Position (This Year till last race)", "Constructor Average Position (This Year till last race)"], axis = 1)

#splitting data into training, validation and testing data based on years
train_df = df_new[(df_new["year"] >= 1983) & (df_new["year"] <= 2008)]
val_df = df_new[(df_new["year"] >= 2009) & (df_new["year"] <= 2016)]
test_df = df_new[(df_new["year"] >= 2017) & (df_new["year"] <= 2023)]

X_train = train_df[train_df.columns.tolist()[:-1]].values
y_train = train_df['Top 3 Finish'].values
X_val = val_df[train_df.columns.tolist()[:-1]].values
y_val = val_df['Top 3 Finish'].values
X_test = test_df[train_df.columns.tolist()[:-1]].values
y_test = test_df['Top 3 Finish'].values

"""This code performs hyperparameter tuning, evaluation, and ROC curve plotting for three machine learning models:

Logistic Regression
Random Forest Classifier
Decision Tree Classifier
"""

# #In this scenario, there is an imbalance between the two classes:
# Class 1: Drivers finishing in the top 3 (only 3 drivers).
# Class 0: Drivers not finishing in the top 3 (more drivers).
# This imbalance means that the majority of data belongs to the "not in top 3" class.
# # Accuracy as a metric would measure the percentage of correct prediction.
# # In class-imbalanced datasets, accuracy can be misleading because predicting the majority class(drivers not in top 3) often results in a high accuracy rate.
# # Example: If the model always predicts "not in top 3," accuracy might be 85%, but the model will fail to identify any driver in the top 3, which is our goal.
#Therefore, we will use AUC-ROC Score will measure how well our model separates 2 classes


# Dictionary to store the best model and test accuracy for each algorithm
model_accuracy_info = {}

# Define the hyperparameter grid for each model
param_grid = {
    'LogisticRegression': {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'random_state': [42]}, #C:Controls regularization strength.
    'RandomForestClassifier': {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20, 30], 'random_state': [42]}, #n_estimators: Number of trees in the forest
    'DecisionTreeClassifier': {'max_depth': [None, 5, 10, 20], 'random_state': [42]} #max_depth: Maximum depth of trees for Decision Tree and Random Forest.
}

# Initialize models
models = {
    'LogisticRegression': LogisticRegression(),
    'RandomForestClassifier': RandomForestClassifier(),
    'DecisionTreeClassifier': DecisionTreeClassifier()
}

# Ignore convergence and future warnings

simplefilter("ignore", category=ConvergenceWarning)
simplefilter("ignore", category=FutureWarning)

# Function to manually tune hyperparameters
def tune_hyperparameters(model, params, X_train, y_train, X_val, y_val):
    best_model = None
    best_params = {}
    best_auc = 0  # Use AUC-ROC instead of F1 score
    for param in ParameterGrid(params):
        model.set_params(**param)
        model.fit(X_train, y_train)
        probabilities = model.predict_proba(X_val) #returns the probability estimates for each class, class 0 and 1
        auc = roc_auc_score(y_val, probabilities[:, 1])  # Calculate AUC-ROC, which measures the modelâ€™s ability to distinguish between the two classes
        if auc > best_auc:
            best_auc = auc
            best_model = model
            best_params = param
    return best_model, best_params

# Perform hyperparameter tuning and evaluation
for name, model in models.items():
    #print(f"Model: {name}")
    if name in param_grid:
        # Tune hyperparameters
        best_model, best_params = tune_hyperparameters(model, param_grid[name], X_train, y_train, X_val, y_val)
        #print(f"Best parameters for {name}: {best_params}")
        model = best_model

    # Predict and evaluate on test data using AUC-ROC
    pred_test = model.predict_proba(X_test)
    auc_test = roc_auc_score(y_test, pred_test[:, 1])  # Calculate AUC-ROC
    accuracy_test = accuracy_score(y_test, pred_test[:, 1] >= 0.5)  # Calculate accuracy

    #print(f"Test AUC-ROC for {name}: {auc_test:.4f}\n")
    model_filename = f"{name}_model_V1.joblib"
    joblib.dump(model, model_filename)
    model_accuracy_info[name] = {
        'model': model_filename,
        'auc_roc': auc_test,  # Store AUC-ROC
        'accuracy': accuracy_test
    }

    # Calculate ROC curve and AUC for each model
    fpr, tpr, thresholds = roc_curve(y_test, pred_test[:, 1])
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.plot(fpr, tpr, label=f'{name} (area = {roc_auc:.2f})')

    # Print model and F1 score info
#for model_name, info in model_accuracy_info.items():
    #print(f"Model: {model_name}, File: {info['model']}, Test AUC-ROC: {info['auc_roc']:.4f}, Test Accuracy: {info['accuracy']:.4f}")

plt.plot([0, 1], [0, 1], 'k--')  # Add a diagonal dashed line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc="lower right")
#plt.show()

"""Class Imbalance: The dataset is imbalanced, where most drivers do not finish in the top 3.
AUC-ROC Score: Measures the model's ability to distinguish between classes (drivers in the top 3 vs. not in the top 3). A higher AUC means better performance.

Therefore, we will use AUC-ROC Score will measure how well our model separates 2 classes

Model 2: Predicting F1 podiums without qualifying data and adding features
"""

#adding feature: 1.Percentages of finishing in top 3 in drivers 2.Percentages of finishing in constructors
df_new = df_final_encoded.drop(["grid", "Driver Average Position (This Year till last race)", "Constructor Average Position (This Year till last race)", "Driver Average Position (This Year till last race)", "Constructor Average Position (This Year till last race)"], axis = 1)
train_df = df_new[(df_new["year"] >= 1983) & (df_new["year"] <= 2008)]
val_df = df_new[(df_new["year"] >= 2009) & (df_new["year"] <= 2016)]
test_df = df_new[(df_new["year"] >= 2017) & (df_new["year"] <= 2023)]

X_train = train_df[train_df.columns.tolist()[:-1]].values
y_train = train_df['Top 3 Finish'].values
X_val = val_df[train_df.columns.tolist()[:-1]].values
y_val = val_df['Top 3 Finish'].values
X_test = test_df[train_df.columns.tolist()[:-1]].values
y_test = test_df['Top 3 Finish'].values

#Dictionary to store the best model and test accuracy for each algorithm
model_accuracy_info = {}

# Define the hyperparameter grid for each model
param_grid = {
    'LogisticRegression': {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'random_state': [42]},
    'RandomForestClassifier': {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20, 30], 'random_state': [42]},
    'DecisionTreeClassifier': {'max_depth': [None, 5, 10, 20], 'random_state': [42]}
}

# Initialize models
models = {
    'LogisticRegression': LogisticRegression(),
    'RandomForestClassifier': RandomForestClassifier(),
    'DecisionTreeClassifier': DecisionTreeClassifier()
}

# Ignore convergence and future warnings
simplefilter("ignore", category=ConvergenceWarning)
simplefilter("ignore", category=FutureWarning)

# Function to manually tune hyperparameters
def tune_hyperparameters(model, params, X_train, y_train, X_val, y_val):
    best_model = None
    best_params = {}
    best_auc = 0  # Use AUC-ROC instead of F1 score
    for param in ParameterGrid(params):
        model.set_params(**param)
        model.fit(X_train, y_train)
        probabilities = model.predict_proba(X_val)
        auc = roc_auc_score(y_val, probabilities[:, 1])  # Calculate AUC-ROC
        if auc > best_auc:
            best_auc = auc
            best_model = model
            best_params = param
    return best_model, best_params

# Perform hyperparameter tuning and evaluation
for name, model in models.items():
    #print(f"Model: {name}")
    if name in param_grid:
        # Tune hyperparameters
        best_model, best_params = tune_hyperparameters(model, param_grid[name], X_train, y_train, X_val, y_val)
        #print(f"Best parameters for {name}: {best_params}")
        model = best_model

    # Predict and evaluate on test data using AUC-ROC
    pred_test = model.predict_proba(X_test)
    auc_test = roc_auc_score(y_test, pred_test[:, 1])  # Calculate AUC-ROC
    accuracy_test = accuracy_score(y_test, pred_test[:, 1] >= 0.5)  # Calculate accuracy

    #print(f"Test AUC-ROC for {name}: {auc_test:.4f}\n")
    model_filename = f"{name}_model_V1.joblib"
    joblib.dump(model, model_filename)
    model_accuracy_info[name] = {
        'model': model_filename,
        'auc_roc': auc_test,  # Store AUC-ROC
        'accuracy': accuracy_test
    }

    # Calculate ROC curve and AUC for each model
    fpr, tpr, thresholds = roc_curve(y_test, pred_test[:, 1])
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.plot(fpr, tpr, label=f'{name} (area = {roc_auc:.2f})')

    # Print model and F1 score info
#for model_name, info in model_accuracy_info.items():
    #print(f"Model: {model_name}, File: {info['model']}, Test AUC-ROC: {info['auc_roc']:.4f}, Test Accuracy: {info['accuracy']:.4f}")

plt.plot([0, 1], [0, 1], 'k--')  # Add a diagonal dashed line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc="lower right")
#plt.show()

"""Model 3: Predicting F1 podiums with qualifying data

"""

df_with_qualifying = df_final_encoded
train_df = df_with_qualifying[(df_with_qualifying["year"] >= 1983) & (df_with_qualifying["year"] <= 2008)]
val_df = df_with_qualifying[(df_with_qualifying["year"] >= 2009) & (df_with_qualifying["year"] <= 2016)]
test_df = df_with_qualifying[(df_with_qualifying["year"] >= 2017) & (df_with_qualifying["year"] <= 2023)]

X_train = train_df[train_df.columns.tolist()[:-1]].values
y_train = train_df['Top 3 Finish'].values
X_val = val_df[train_df.columns.tolist()[:-1]].values
y_val = val_df['Top 3 Finish'].values
X_test = test_df[train_df.columns.tolist()[:-1]].values
y_test = test_df['Top 3 Finish'].values

#Dictionary to store the best model and test accuracy for each algorithm
model_accuracy_info = {}

# Define the hyperparameter grid for each model
param_grid = {
    'LogisticRegression': {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'random_state': [42]},
    'RandomForestClassifier': {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20, 30], 'random_state': [42]},
    'DecisionTreeClassifier': {'max_depth': [None, 5, 10, 20], 'random_state': [42]}
}

# Initialize models
models = {
    'LogisticRegression': LogisticRegression(),
    'RandomForestClassifier': RandomForestClassifier(),
    'DecisionTreeClassifier': DecisionTreeClassifier()
}

# Ignore convergence and future warnings
simplefilter("ignore", category=ConvergenceWarning)
simplefilter("ignore", category=FutureWarning)

# Function to manually tune hyperparameters
def tune_hyperparameters(model, params, X_train, y_train, X_val, y_val):
    best_model = None
    best_params = {}
    best_auc = 0  # Use AUC-ROC instead of F1 score
    for param in ParameterGrid(params):
        model.set_params(**param)
        model.fit(X_train, y_train)
        probabilities = model.predict_proba(X_val)
        auc = roc_auc_score(y_val, probabilities[:, 1])  # Calculate AUC-ROC
        if auc > best_auc:
            best_auc = auc
            best_model = model
            best_params = param
    return best_model, best_params

# Perform hyperparameter tuning and evaluation
for name, model in models.items():
    #print(f"Model: {name}")
    if name in param_grid:
        # Tune hyperparameters
        best_model, best_params = tune_hyperparameters(model, param_grid[name], X_train, y_train, X_val, y_val)
        #print(f"Best parameters for {name}: {best_params}")
        model = best_model

    # Predict and evaluate on test data using AUC-ROC
    pred_test = model.predict_proba(X_test)
    auc_test = roc_auc_score(y_test, pred_test[:, 1])  # Calculate AUC-ROC
    accuracy_test = accuracy_score(y_test, pred_test[:, 1] >= 0.5)  # Calculate accuracy

    #print(f"Test AUC-ROC for {name}: {auc_test:.4f}\n")
    model_filename = f"{name}_model_V1.joblib"
    joblib.dump(model, model_filename)
    model_accuracy_info[name] = {
        'model': model_filename,
        'auc_roc': auc_test,  # Store AUC-ROC
        'accuracy': accuracy_test
    }

    # Calculate ROC curve and AUC for each model
    fpr, tpr, thresholds = roc_curve(y_test, pred_test[:, 1])
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.plot(fpr, tpr, label=f'{name} (area = {roc_auc:.2f})')

    # Print model and F1 score info
#for model_name, info in model_accuracy_info.items():
    #print(f"Model: {model_name}, File: {info['model']}, Test AUC-ROC: {info['auc_roc']:.4f}, Test Accuracy: {info['accuracy']:.4f}")

plt.plot([0, 1], [0, 1], 'k--')  # Add a diagonal dashed line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc="lower right")
#plt.show()

"""
Model Interpretation"""

max_depth = 2 # Adjust this value to control the number of levels shown

dt_model = joblib.load("DecisionTreeClassifier_model_V1.joblib")

# Set the size of the plot
plt.figure(figsize=(70, 20))


dt_feature_importances = dt_model.feature_importances_

# Create a DataFrame from the feature names and their importances
dt_importances_df = pd.DataFrame({
    'Feature': train_df.columns[:-1],  # Ensure this matches your feature set
    'Importance': dt_feature_importances
})

# Sort the DataFrame by the importances
dt_importances_df.sort_values(by='Importance', ascending=False, inplace=True)

# Print the sorted DataFrame
#print(dt_importances_df.head(10))

model = joblib.load("RandomForestClassifier_model_V1.joblib")
importances = model.feature_importances_

# Create a DataFrame for visualization
importances_df = pd.DataFrame({'Feature': df_with_qualifying.columns[:-1], 'Importance': importances})

# Sort the DataFrame
importances_df.sort_values(by='Importance', ascending=False, inplace=True)

# Display the feature importances
#print(importances_df.head(10))

#print(model.get_params(deep = True))

from sklearn.metrics import precision_recall_curve, f1_score, ConfusionMatrixDisplay
# Load the model
model = joblib.load("RandomForestClassifier_model_V1.joblib")

# Predict probabilities on new data
data_prob = model.predict_proba(X_test)[:, 1]

# Compute Precision-Recall curve
precision, recall, thresholds = precision_recall_curve(y_test, data_prob)
f1_scores = 2 * recall * precision / (recall + precision + 1e-10)

# Find the optimal threshold on f1 since using the default threshold of 0.5 can often lead to misleading results
optimal_idx = np.nanargmax(f1_scores)  # Using nanargmax to ignore NaN values
optimal_threshold = thresholds[optimal_idx]
#print(f"Optimal Threshold for F1 Score: {optimal_threshold:.3f}")

# Apply the new threshold to make class predictions
pred_test = (data_prob >= optimal_threshold).astype(int)

# Display the Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_test, pred_test, display_labels=['Class 0', 'Class 1'])
plt.title("Confusion Matrix for Random Forest Classifier on Test Data")
#plt.show()

# Calculate and print the accuracy
accuracy = accuracy_score(y_test, pred_test)
#print(f"Accuracy of the model: {accuracy:.3f}")

# Calculate and print the F1 score
f1 = f1_score(y_test, pred_test)
#print(f"F1 Score of the model: {f1:.3f}")

# Create a dictionary to hold the model and the threshold
model_data = {
    "model": model,
    "threshold": optimal_threshold
}

# Save the dictionary
model_filename = "RandomForestClassifier_model_V4_with_threshold.joblib"
joblib.dump(model_data, model_filename)

#Precision tells you how many of the instances classified as Class 1 are actually correct.
#Recall tells you how many of the actual Class 1 instances were correctly classified.
#F1 score is the harmonic mean of precision and recall, providing a balanced measure of model performance.

"""Confusion matrix analysis:
True Negatives (TN): 1728
This represents the number of times the model correctly predicted Class 0 (i.e., drivers who did not finish in the top 3).

False Positives (FP): 197
These are the times the model incorrectly predicted Class 1 (drivers in the top 3) when they were actually Class 0.

False Negatives (FN): 93
These are the times the model incorrectly predicted Class 0 when the actual class was Class 1 (i.e., it missed predicting the top 3 finishers).

True Positives (TP): 315
This is the number of times the model correctly predicted Class 1 (drivers who finished in the top 3).

Readings:
The model has a high accuracy (87.6%), but its performance on Class 1 (drivers in the top 3) could be improved, as seen from the 93 False Negatives and 197 False Positives.
The Precision for Class 1 is 0.615, meaning that out of all instances predicted as top 3, 61.5% were actually correct.
The Recall for Class 1 is 0.773, indicating that 77.3% of the actual top 3 finishers were identified by the model.

F1 podium prediction example: 2023 Spanish Grand Prix (Round 7)
"""

round = 7

# Load the model and threshold
model_data = joblib.load("RandomForestClassifier_model_V4_with_threshold.joblib")
model = model_data["model"]
optimal_threshold = model_data["threshold"]

# Filter the DataFrame for the year 2023 and round 7 (Spanish Grand Prix)
df_2023 = df_with_qualifying[(df_with_qualifying["year"] == 2023) & (df_with_qualifying["round"] == round)]

# Prepare the feature matrix for prediction
X_2023 = df_2023[df_2023.columns.tolist()[:-1]].values

# Compute probabilities using the model
probabilities = model.predict_proba(X_2023)

# Apply the threshold to make class predictions
pred_test = (probabilities[:, 1] >= optimal_threshold).astype(int)  # Assuming the second column represents the probability of top 3 finish

# Selecting relevant columns and filtering for year 2023 and round 7
df_predict2023 = df_final[["year", "round", "driverId", "constructorId", "grid", "Top 3 Finish"]]
df_predict2023 = df_predict2023[(df_predict2023["year"] == 2023) & (df_predict2023["round"] == round)]



# Create a dictionary to map 'driverId' to 'surname'
driver_name_dict = pd.Series(drivers_df.surname.values, index=drivers_df.driverId).to_dict()

# Replace 'driverId' with corresponding 'surname' in df_predict2023
df_predict2023['driverId'] = df_predict2023['driverId'].map(driver_name_dict)

# Append the predictions to the DataFrame
df_predict2023['Top_3_Finish_Prediction'] = pred_test


# Create a dictionary to map 'constructorId' to 'name'
constructor_name_dict = pd.Series(constructors_df.name.values, index=constructors_df.constructorId).to_dict()

# Replace 'constructorId' with corresponding 'name' in df_predict2023
df_predict2023['constructorId'] = df_predict2023['constructorId'].map(constructor_name_dict)

# Append the predictions to the DataFrame
df_predict2023['Top_3_Finish_Prediction'] = pred_test


# Print the DataFrame with predictions
#print(df_predict2023)

"""F1 podium prediction example: 2024 Italian Grand Prix (Round 7)

"""

round = 7
year = 2024
# Load the model and threshold
model_data = joblib.load("RandomForestClassifier_model_V4_with_threshold.joblib")
model = model_data["model"]
optimal_threshold = model_data["threshold"]

# Filter the DataFrame for the year 2023 and round 7 (Spanish Grand Prix)
df_year = df_with_qualifying[(df_with_qualifying["year"] == year) & (df_with_qualifying["round"] == round)]

# Prepare the feature matrix for prediction
X_year = df_year[df_year.columns.tolist()[:-1]].values

# Compute probabilities using the model
probabilities = model.predict_proba(X_year)

# Apply the threshold to make class predictions
pred_test = (probabilities[:, 1] >= optimal_threshold).astype(int)  # Assuming the second column represents the probability of top 3 finish

# Selecting relevant columns and filtering for year 2024 and round 7
df_predictyear = df_final[["year", "round", "driverId", "constructorId", "grid", "Top 3 Finish"]]
df_predictyear = df_predictyear[(df_predictyear["year"] == year) & (df_predictyear["round"] == round)]



# Create a dictionary to map 'driverId' to 'surname'
driver_name_dict = pd.Series(drivers_df.surname.values, index=drivers_df.driverId).to_dict()

# Replace 'driverId' with corresponding 'surname' in df_predict2023
df_predictyear['driverId'] = df_predictyear['driverId'].map(driver_name_dict)

# Append the predictions to the DataFrame
df_predictyear['Top_3_Finish_Prediction'] = pred_test


# Create a dictionary to map 'constructorId' to 'name'
constructor_name_dict = pd.Series(constructors_df.name.values, index=constructors_df.constructorId).to_dict()

# Replace 'constructorId' with corresponding 'name' in df_predict2023
df_predictyear['constructorId'] = df_predictyear['constructorId'].map(constructor_name_dict)

# Append the predictions to the DataFrame
df_predictyear['Top_3_Finish_Prediction'] = pred_test


# Print the DataFrame with predictions
#print(df_predictyear)

def process_predictions(round, year):
    import joblib
    import pandas as pd

    # Load the model and threshold
    model_data = joblib.load("RandomForestClassifier_model_V4_with_threshold.joblib")
    model = model_data["model"]
    optimal_threshold = model_data["threshold"]

    # Filter the DataFrame for the given year and round
    df_year = df_with_qualifying[(df_with_qualifying["year"] == year) & (df_with_qualifying["round"] == round)]

    # Prepare the feature matrix for prediction
    X_year = df_year[df_year.columns.tolist()[:-1]].values

    # Compute probabilities using the model
    probabilities = model.predict_proba(X_year)

    # Apply the threshold to make class predictions
    pred_test = (probabilities[:, 1] >= optimal_threshold).astype(int)  # Assuming the second column represents the probability of top 3 finish

    # Selecting relevant columns and filtering for the given year and round
    df_predictyear = df_final[["year", "round", "driverId", "constructorId", "grid", "Top 3 Finish"]]
    df_predictyear = df_predictyear[(df_predictyear["year"] == year) & (df_predictyear["round"] == round)]

    # Create a dictionary to map 'driverId' to 'surname'
    driver_name_dict = pd.Series(drivers_df.surname.values, index=drivers_df.driverId).to_dict()

    # Replace 'driverId' with corresponding 'surname' in df_predictyear
    df_predictyear['driverId'] = df_predictyear['driverId'].map(driver_name_dict)

    # Create a dictionary to map 'constructorId' to 'name'
    constructor_name_dict = pd.Series(constructors_df.name.values, index=constructors_df.constructorId).to_dict()

    # Replace 'constructorId' with corresponding 'name' in df_predictyear
    df_predictyear['constructorId'] = df_predictyear['constructorId'].map(constructor_name_dict)

    # Append the predictions to the DataFrame
    df_predictyear['Top_3_Finish_Prediction'] = pred_test

    # Return the DataFrame with predictions
    return df_predictyear

# # Example usage
# if __name__ == "__main__":
#     round_input = 7
#     year_input = 2024
#     predictions = process_predictions(round_input, year_input)
#     #print(predictions)
